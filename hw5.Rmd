---
title: "hw5"
author: "Jingchen Chai"
output: 
     pdf_document:
         latex_engine: xelatex
geometry: margin=2cm
date: "2022-12-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

```{r}
library(tidyverse)
library(patchwork)
library(modelr)
library(leaps)
library(purrr)
library(glmnet)
library(knitr)
library(caret)
```

```{r}
state= state.x77 %>%
  as.tibble()%>%
  janitor::clean_names()%>%
  select(life_exp,everything()) 


```

## a)
```{r}

sum = function(variable){
  tibble(
    mean = mean(variable),
    sd = sd(variable),
    median = median(variable),
    maximum = max(variable),
    minimum = min(variable),
    IQR = IQR(variable)
  )
}

map(state, sum) %>% 
  bind_rows() %>% 
  mutate(variable = names(state)) %>% 
  select(variable,everything()) %>%
  knitr::kable(digits = 2, 
               caption = "Descriptive statistics of continuous variables")
```

## b)

```{r}
plot(state)

cor(state) %>%
  knitr::kable(digits=2,caption="Correlation for all variables")

```

```{r}
state%>% select(-life_exp)%>%
  funModeling::plot_num()
```

From the above plot, we can see that population and area are skewed, while all other variables are pretty normal distributed. We would want to try to make transformations on population and area.

```{r}
ggl_p = 
state %>% 
  mutate(population = log(population)) %>% 
 ggplot(aes(x=population,y=..density..))+
 geom_histogram()+
 geom_line(stat = 'density')+
    labs(x = "population")
ggl_p

ggl_a=state %>%
  mutate(area = log(area)) %>% 
 ggplot(aes(x=area,..density..))+
 geom_histogram()+geom_line(stat = 'density')+
    labs(x = "area")
ggl_p+ggl_a
 
```

```{r fit with all predictors}
multi.fit=lm(life_exp ~ ., data = state) 
summary(multi.fit)

```

1) __Method I: Backward elimination__

By looking at the summary of full model regression, backward elimination starts eliminating the one with largest p value, we stop remove variables when their p-value are all less than 0.05. so we __remove area__ first

```{r remove area}
step1 <- update(multi.fit, . ~ . -area)
summary(step1)
```

Then we __remove illiteracy__

```{r remove illiteracy}
step2 <- update(step1, . ~ . -illiteracy)
summary(step2)
```

Then we __remove income__

```{r}
step3 <- update(step2, . ~ . -income)
summary(step3)
```

Then we __remove population__

```{r}
step4 <- update(step3, . ~ . -population)
summary(step4)
```

__Result__: backward selection model is 

life expectancy = 71 - 0.3Murder + 0.047hs_grad - 0.006frost

2) __Method II: Forward elimination__

```{r}
variable=names(state)

map(.x=variable,~lm(substitute(life_exp ~ i, list(i = as.name(.x))), data = state)) %>%
  map_df(.,broom::tidy)%>%
  filter(term!="(Intercept)") %>%
  select(term,p.value)%>%
  arrange(p.value)
```

So we first enter the one with the lowest p-value 2.26e-11 < 0.05: `murder`.

```{r, warning=F}
forward1 = lm(life_exp ~ murder, data = state) 
summary(forward1)
```


```{r, warning=F}
variable=names(state)
map(.x = variable, ~update(forward1, substitute(.~. + i, list(i = as.name(.x))))) %>% 
  map_df(., broom::tidy) %>% 
  filter(term != "(Intercept)", term != "murder") %>% 
  select(term,p.value) %>%
  arrange(p.value)
```
Enter the one with the lowest p-value 0.00909: `hs_grad`.
```{r, warning=F}
forward2 <- update(forward1, . ~ . + hs_grad)
summary(forward2)
```

```{r}
variable=names(state)
map(.x = variable, ~update(forward2, substitute(.~. + i, list(i = as.name(.x))))) %>% 
  map_df(., broom::tidy) %>% 
  filter(term != "(Intercept)", term != "murder",term!="hs_grad") %>% 
  arrange(p.value)

```

Enter the one with the lowest p-value 0.00699: `frost`.
```{r, warning=F}
forward3 <- update(forward2, . ~ . + frost) 
summary(forward3)
```

```{r}
variable=names(state)
map(.x = variable, ~update(forward3, substitute(.~. + i, list(i = as.name(.x))))) %>% 
  map_df(., broom::tidy) %>% 
  filter(term != "(Intercept)", term != "murder",term!="hs_grad",term!="frost") %>% 
  arrange(p.value)

```

P-value of all new added variables are larger than 0.05, which means that they are not significant predictor, so we stop here.

```{r, warning=F}
forward_fit = lm(life_exp ~ murder + hs_grad + frost, data = state) %>%
summary() %>% broom::tidy()
```

The model we obtained by forward elimination is life_exp ~ murder + hs_grad + frost.

__Method III: stepwise regression__
```{r}
step.fit <- lm(life_exp ~ ., data = state)
step(step.fit, direction = 'both') # select by AIC 
```
We choose the one with smallest AIC, hence the model selected by stepwise regression procedure is:  

life_exp = 71 + 0.00005population - 0.3murder + 0.047hs_grad - 0.006frost


* Do the procedures generate the same model?

Backward elimination and forward elimination generated the same model: `life_exp ~ murder + hs_grad + frost`. However, stepwise regression generated a larger model with add a pridictor `population`.

* Is there any variable a close call? What was your decision: keep or discard? Provide
arguments for your choice.

The variable `population` is a close call, with p-value = 0.052. I would keep it, because its p-value is close to 0.05. This model has a better AIC than a smaller model. And adding 'population' contributes to the goodness of fit by increasing the adjusted R2 from 0.6939 to 0.7126. 

* Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’
contain both?

The correlation coefficient between ‘Illiteracy’ and ‘HS graduation rate’ is -0.66, indicating a moderate association. My subset only contains 'HS graduate rate". ‘Illiteracy’ is not included.

## d)
```{r}
leaps(x = state %>% select(-life_exp), y = state[[1]], nbest = 1, method = "Cp")
```

```{r}
leaps(x = state %>% select(-life_exp), y = state[[1]], nbest = 1, method = "adjr2")

```

```{r}
sub = regsubsets(life_exp ~ ., data = state)
summ=summary(sub) 

```

```{r}
plot_cp = 
  tibble(x = 2:8, y = summ$cp) %>% 
  ggplot(aes(x = x, y = y)) +
    geom_point() + geom_line()+
    labs(x = "# parameters", y = "Cp")

plot_adjr2 = 
  tibble(x = 2:8, y = summ$adjr2) %>% 
  ggplot(aes(x = x, y = y)) +
    geom_point() + geom_line()+
    labs(x = "# parameters", y = "Adjusted R2")
plot_cp + plot_adjr2
```

Based on the Cp and adjusted R2 criterion, I would choose the 4-predictors (5 parameters) model. The best 4-predictors model is life_exp ~ population + murder + hs_grad + frost. It has the highest adjusted R2 and the lowest Cp value.

## e)

```{r}
lambda_seq <- 10^seq(-3, 0, by = .1)
set.seed(2022)
cv_object <- cv.glmnet(as.matrix(state[2:8]), state$life_exp,
lambda = lambda_seq,
nfolds = 5)
cv_object

```

```{r}
cv_object$lambda.min
tibble(lambda = cv_object$lambda,
mean_cv_error = cv_object$cvm) %>%
ggplot(aes(x = lambda, y = mean_cv_error)) +
geom_point()
```
When lambda is 0.1995, it will generate the lowest cv error. 

Now I will try to refit the model with the best lambda.
```{r}
fit_bestcv <- glmnet(as.matrix(state[2:8]), state$life_exp, lambda = cv_object$lambda.min)
coef(fit_bestcv)
```
The results shows that `murder` and `hs_grad` should be included in the model.

## f)

### Diagnostics

Based on the models chosen from c), d), and e), forward elimination, stepwise regression, and the criterion-based procedures, they all ended up with the model: life_exp~ population + murder+ hs_grad + frost. I will use this as my final model.
```{r}
final_model=lm(life_exp~population+murder+hs_grad+frost,data=state)
par(mfrow=c(2,2))
plot(final_model)

```

The residuals scattered evenly along the fitted values. We can assume that residuals have a mean of 0, and a constant variance, and independent of each other. The QQ plot shows that the tail slightly deviates from the straight line, this may indicates that there exists outliers. Additionally, there is no influential observations according to the residual vs leverage plot.

### 10-fold Cross Validation

```{r}
train_control = trainControl(method ="cv", number = 10, savePredictions = TRUE)
 
model = train(life_exp ~ murder + hs_grad + frost + population, 
              data = state, 
              trControl = train_control, 
              method = 'lm')
model
predictions = model$resample
```

For 10-fold cross-validation, the `RMSE` is about 0.74, `Rsquare` is about 0.73, `MAE` is about 0.62. There are about 73% of variation explained by our model. 

## g)

The final model that I chose is life_exp ~ population + murder + hs_grad + frost. Some predictors are known as junk predictors, which will not provide help for us to predict life expectancy. The four predictors that was included, shows statistically significant when predicting life expectancy. After choosing the model, I tried to validate it. The R square shows to be 0.73, which means the model explained pretty well on the observed data.     
